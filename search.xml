<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>CUDA学习(五)</title>
      <link href="/2024/02/24/CUDA%E5%AD%A6%E4%B9%A0-%E4%BA%94/"/>
      <url>/2024/02/24/CUDA%E5%AD%A6%E4%B9%A0-%E4%BA%94/</url>
      
        <content type="html"><![CDATA[<p>本章将学习如何分配和使用纹理内存(texture memory)。和常量内存一样，纹理内存是另一种类型的只读内存，在特定的访问模式中，纹理内存同样能够提升性能并减少内存流量。虽然纹理内存最初是针对传统的图形处理应用程序而设计的，但在某些GPU计算应用程序中同样非常有用。</p><!---more---><p>与常量内存相似的是，纹理内存同样缓存在芯片上，因此在某些情况中，它能够减少对内存请求并提供更高效的内存带宽。纹理缓存是专门为那些在内存访问模式中存在大量空间局部性(spatial locality)的图形应用程序而设计的。在某个计算应用程序中，这意味着一个线程读取的位置可能与邻近线程读取的位置“非常接近”。</p><h2 id="热传导模拟"><a href="#热传导模拟" class="headerlink" title="热传导模拟"></a>热传导模拟</h2><p>本章用一个简单的热传导模拟的模型来介绍如何使用纹理内存。</p><h3 id="简单的传热模型"><a href="#简单的传热模型" class="headerlink" title="简单的传热模型"></a>简单的传热模型</h3><p>我们构造一个简单的二维热传导模拟。首先假设有一个矩形房间，将其分成一个格网，每个格网中随机散步一些“热源”，他们有着不同的温度。</p><p><img src="/imgs/CUDA_5_1.png" alt="CUDA_5_1"></p><p>在给定了矩形格网以及热源分布后，我们可以计算格网中每个单元格的温度随时间的变化情况。为了简单，热源单元本身的温度将保持不变。在时间递进的每个步骤中，我们假设热量在某个单元及其邻接单元之间“流动”。如果某个单元的临界单元的温度更高，那么热量将从邻接单元传导到该单元，相反地，如果某个单元的温度比邻接单元的温度高，那么它将变冷。</p><p>在热传导模型中，我们对单元中新温度的计算方法为，将单元与邻接单元的温差相加起来，然后加上原有温度。<br>$$<br>T_{NEW} &#x3D; T_{OLD} + \sum_{NEIGHBOURS}k(T_{NEIGHBORS} - T_{OLD})<br>$$<br>在上面的计算单元温度的等式中，常量k表示模拟过程中热量的流动速率，k值越大，表示系统会更快地达到稳定温度，而k值越小，则温度梯度将存在更长时间。由于我们只考虑4个邻接单元(上、下、左、右)并且等式中的k和$$T_{OLD}$$都是常数，因此把上述公式展开表示为:<br>$$<br>T_{NEW} &#x3D; T_{OLD} + k(T_{TOP} + T_{BOTTOM} + T_{LEFT} + T_{RIGHT} - 4T_{OLD})<br>$$</p><h3 id="温度更新的计算"><a href="#温度更新的计算" class="headerlink" title="温度更新的计算"></a>温度更新的计算</h3><p>以下是更新流程的基本介绍:</p><ol><li>给定一个包含初始输入温度的格网，将其中作为热源的单元温度值复制到格网相应的单元中来覆盖这些单元之前计算出来的温度，确保“加热单元将保持恒温”的条件。这个复制操作在copy_const_kernel()中执行。</li><li>给定一个输入温度格网，根据新的公式计算出输出温度格网。这个更新操作在blend_kernel()中执行。</li><li>将输入温度格网和输出温度格网交换，为下一个步骤的计算做好准备。当模拟下一个时间步时，在步骤2中计算得到的输出温度格网将成为步骤1中的输入温度格网。</li></ol><p>下面是两个函数的具体实现:</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">__global__ <span class="type">void</span> <span class="title function_">copy_const_kernel</span><span class="params">(<span class="type">float</span> *iptr, <span class="type">const</span> <span class="type">float</span> *cptr)</span>&#123;</span><br><span class="line">    <span class="comment">//将threadIdx/BlockIdx映射到像素位置</span></span><br><span class="line">    <span class="type">int</span> x = threadIdx.x + blockIdx.x * blockDim.x;</span><br><span class="line">    <span class="type">int</span> y = threadIdx.y + blockIdx.y * blockDim.y;</span><br><span class="line">    <span class="type">int</span> offset = x + y * blockDim.x * gridDim.x;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span>(cptr[offset] != <span class="number">0</span>)&#123;</span><br><span class="line">        iptr[offset] = cptr[offset];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//为了执行更新操作，可以在模拟过程中让每个线程都负责计算一个单元。</span></span><br><span class="line"><span class="comment">//每个线程都将读取对应单元及其邻接单元的温度值，执行更新运算，然后计算得到新值来更新温度。</span></span><br><span class="line">__global__ <span class="type">void</span> <span class="title function_">blend_kernel</span><span class="params">(<span class="type">float</span> *outSrc, <span class="type">const</span> <span class="type">float</span> *inSrc)</span>&#123;</span><br><span class="line">    <span class="comment">//将threadIdx/BlockIdx映射到像素位置</span></span><br><span class="line">    <span class="type">int</span> x = threadIdx.x + blockIdx.x * blockDim.x;</span><br><span class="line">    <span class="type">int</span> y = threadIdx.y + blockIdx.y * blockDim.y;</span><br><span class="line">    <span class="type">int</span> offset = x + y * blockDim.x * gridDim.x;</span><br><span class="line">    </span><br><span class="line">    <span class="type">int</span> left = offset - <span class="number">1</span>;</span><br><span class="line">    <span class="type">int</span> right = offset + <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">if</span>(x == <span class="number">0</span>) left++;</span><br><span class="line">    <span class="keyword">if</span>(x == DIM - <span class="number">1</span>) right--;</span><br><span class="line">    </span><br><span class="line">    <span class="type">int</span> top = offset - DIM;</span><br><span class="line">    <span class="type">int</span> bottom = offset + DIM;</span><br><span class="line">    <span class="keyword">if</span>(y == <span class="number">0</span>) bottom += DIM;</span><br><span class="line">    <span class="keyword">if</span>(y == DIM - <span class="number">1</span>) top -= DIM;</span><br><span class="line">    </span><br><span class="line">    outSrc[offset] = inSrc[offset] + SPEED * (inSrc[top] + inSrc[bottom] + inSrc[left] + inSrc[right] - inSrc[offset] * <span class="number">4</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="模拟过程动态演示"><a href="#模拟过程动态演示" class="headerlink" title="模拟过程动态演示"></a>模拟过程动态演示</h3><p>剩下的代码主要是设置好单元，然后显示热量的动画输出</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;cuda.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;book.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;cpu_anim.h&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> DIM 1024</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> PI 3.1415926535897932f</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> MAX_TEMP 1.0f</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> MIN_TEMP 0.0001f</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> SPEED 0.25f</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//更新函数中需要的全局变量</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">DataBlock</span>&#123;</span></span><br><span class="line">    <span class="type">unsigned</span> <span class="type">char</span> *output_bitmap;</span><br><span class="line">    <span class="type">float</span> *dev_inSrc;</span><br><span class="line">    <span class="type">float</span> *dev_outSrc;</span><br><span class="line">    <span class="type">float</span> *dev_constSrc;</span><br><span class="line">    CPUAnimBitmap *bitmap;</span><br><span class="line">    </span><br><span class="line">    cudaEvent_t start, stop;</span><br><span class="line">    <span class="type">float</span> totalTime;</span><br><span class="line">    <span class="type">float</span> frames;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">anim_gpu</span><span class="params">(DataBlock *d, <span class="type">int</span> ticks)</span>&#123;</span><br><span class="line">    HANDLE_ERROR(cudaEventRecord(d-&gt;start, <span class="number">0</span>));</span><br><span class="line">    dim3 <span class="title function_">blocks</span><span class="params">(DIM / <span class="number">16</span>, DIM / <span class="number">16</span>)</span>;</span><br><span class="line">    dim3 <span class="title function_">threads</span><span class="params">(<span class="number">16</span>, <span class="number">16</span>)</span>;</span><br><span class="line">    CPUAnimBitmap *bitmap = d-&gt;bitmap;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">90</span>; <span class="number">0</span>++)&#123;</span><br><span class="line">        copy_const_kernel&lt;&lt;&lt;blocks, threads&gt;&gt;&gt;(d-&gt;dev_inSrc, d-&gt;dev_constSrc);</span><br><span class="line">        blend_kernel&lt;&lt;&lt;blocks, threads&gt;&gt;&gt;(d-&gt;dev_inSrc, d-&gt;dev_constSrc);</span><br><span class="line">        swap(d-&gt;dev_inSrc, d-&gt;dev_outSrc);</span><br><span class="line">    &#125;</span><br><span class="line">    float_to_color&lt;&lt;&lt;blocks, threads&gt;&gt;&gt;(d-&gt;output_bitmap, d-&gt;dev_inSrc);</span><br><span class="line">    </span><br><span class="line">    HANDLE_ERROR(cudaMemcpy(bitmap-&gt;get_ptr(), d-&gt;output_bitmap, bitmap-&gt;image_size(), cudaMemcpyDeviceToHost));</span><br><span class="line">    HANDLE_ERROR(cudaEventRecord(d-&gt;stop, <span class="number">0</span>));</span><br><span class="line">    HANDLE_ERROR(cudaEventSynchronize(d-&gt;stop));</span><br><span class="line">    </span><br><span class="line">    <span class="type">float</span> elapsedTime;</span><br><span class="line">    HANDLE_ERROR(cudaEventElapsedTime(&amp;elapsedTime, d-&gt;start, d-&gt;stop));</span><br><span class="line">    </span><br><span class="line">    d-&gt;totalTime += elapsedTime;</span><br><span class="line">    ++d-&gt;frames;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Averaged Time per frame: $3.1f ms \n&quot;</span>, d-&gt;totalTime / d-&gt;frames);</span><br><span class="line">    </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">anim_exit</span><span class="params">(DataBlock *d)</span>&#123;</span><br><span class="line">    cudaFree(d-&gt;dev_inSrc);</span><br><span class="line">    cudaFree(d-&gt;dev_outSrc);</span><br><span class="line">    cudaFree(d-&gt;dev_constSrc);</span><br><span class="line">    </span><br><span class="line">    HANDLE_ERROR(cudaEventDestroy(d-&gt;start));</span><br><span class="line">    HANDLE_ERROR(cudaEventDestroy(d-&gt;stop));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span>&#123;</span><br><span class="line">    DataBlock data;</span><br><span class="line">    CPUAnimBitmap <span class="title function_">bitmap</span><span class="params">(DIM, DIM, &amp;data)</span>;</span><br><span class="line">    data.bitmap = &amp;bitmap;</span><br><span class="line">    data.totalTime = <span class="number">0</span>;</span><br><span class="line">    data.frames = <span class="number">0</span>;</span><br><span class="line">    </span><br><span class="line">    HANDLE_ERROR(cudaEventCreate(&amp;data.start));</span><br><span class="line">    HANDLE_ERROR(cudaEventCreate(&amp;data.stop));</span><br><span class="line">    HANDLE_ERROR(cudaMalloc((<span class="type">void</span>**)&amp;data.output_bitmap, bitmap.image_size()));</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//假设float类型的大小为4个字符(即rgba)</span></span><br><span class="line">    HANDLE_ERROR(cudaMalloc((<span class="type">void</span>**)&amp;data.dev_inSrc, bitmap.image_size()));</span><br><span class="line">    HANDLE_ERROR(cudaMalloc((<span class="type">void</span>**)&amp;data.dev_outSrc, bitmap.image_size()));</span><br><span class="line">    HANDLE_ERROR(cudaMalloc((<span class="type">void</span>**)&amp;data.dev_constSrc, bitmap.image_size()));</span><br><span class="line">    </span><br><span class="line">    <span class="type">float</span> *temp = (<span class="type">float</span>*)<span class="built_in">malloc</span>(bitmap.image_size());</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i &lt; DIM*DIM; i++)&#123;</span><br><span class="line">        temp[i] = <span class="number">0</span>;</span><br><span class="line">        <span class="type">int</span> x = i % DIM;</span><br><span class="line">        <span class="type">int</span> y =i / DIM;</span><br><span class="line">        <span class="keyword">if</span>((x &gt; <span class="number">300</span>) &amp;&amp; (x &lt; <span class="number">600</span>) &amp;&amp; (y &gt; <span class="number">310</span>) &amp;&amp; (y &lt; <span class="number">601</span>))&#123;</span><br><span class="line">            temp[i] = MAX_TEMP;</span><br><span class="line">        &#125;</span><br><span class="line">        temp[DIM * <span class="number">100</span> + <span class="number">100</span>] = (MAX_TEMP + MIN_TEMP) / <span class="number">2</span>;</span><br><span class="line">        temp[DIM * <span class="number">700</span> + <span class="number">100</span>] = MIN_TEMP;</span><br><span class="line">        temp[DIM * <span class="number">300</span> + <span class="number">300</span>] = MIN_TEMP;</span><br><span class="line">        temp[DIM * <span class="number">200</span> + <span class="number">700</span>] = MIN_TEMP;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> y = <span class="number">800</span>; y &lt; <span class="number">900</span>; y++)&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="type">int</span> x = <span class="number">400</span>; x &lt; <span class="number">500</span>; x++)&#123;</span><br><span class="line">                temp[x * y * DIM] = MIN_TEMP;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        HANDLE_ERROR(cudaMemcpy(data.dev_constSrc, temp, bitmap.image_size(), cudaMemcpyHostToDevice));</span><br><span class="line">        </span><br><span class="line">        <span class="built_in">free</span>(temp);</span><br><span class="line">        </span><br><span class="line">        bitmap.anim_and_exit((<span class="type">void</span> (*) (<span class="type">void</span>*, <span class="type">int</span>)) anim_gpu, (<span class="type">void</span> (*) (<span class="type">void</span>*)) anim_exit);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="使用纹理内存"><a href="#使用纹理内存" class="headerlink" title="使用纹理内存"></a>使用纹理内存</h3>]]></content>
      
      
      <categories>
          
          <category> CUDA </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>CUDA学习(四)</title>
      <link href="/2024/02/22/CUDA%E5%AD%A6%E4%B9%A0-%E5%9B%9B/"/>
      <url>/2024/02/22/CUDA%E5%AD%A6%E4%B9%A0-%E5%9B%9B/</url>
      
        <content type="html"><![CDATA[<p>这一章会介绍如何在CUDA C中使用常量内存、了解常量内存的性能特性以及学习如何使用CUDA事件来测量应用程序的性能。</p><!---more---><h2 id="常量内存"><a href="#常量内存" class="headerlink" title="常量内存"></a>常量内存</h2><p>到目前为止，我们知道CUDA C程序中可以使用全局内存和共享内存。但是，CUDA C还支持另一种类型的内存，即常量内存。常量内存用于保存在核函数执行期间不会发生变化的数据。在某些情况下，用常量内存来替换全局内存能有效地减少内存带宽。</p><h3 id="在GPU上实现光线跟踪"><a href="#在GPU上实现光线跟踪" class="headerlink" title="在GPU上实现光线跟踪"></a>在GPU上实现光线跟踪</h3><p>我们给出一个简单的光线跟踪应用程序示例来学习。由于OpenGL和DirectX等API都不是专门为了实现光线跟踪而设计的，因此我们必须使用CUDA C来实现基本的光线跟踪器。本示例构造的光线跟踪器非常简单，旨在学习常量内存的使用上(并不能通过示例代码来构建一个功能完备的渲染器)。这个光线跟踪器只支持一组包含球状物体的场景，并且相机被固定在了Z轴，面向原点。此外，示例代码也不支持场景中的任何照明，从而避免二次光线带来的复杂性。代码也不计算照明效果，而只是为每个球面分配一个颜色值，如果它们是可见的，则通过某个预先计算的值对其着色。</p><p>光线跟踪器将从每个像素发射一道光线，并且跟踪到这些光线会命中哪些球面。此外，它还将跟踪每道命中光线的深度。当一道光线穿过多个球面时，只有最接近相机的球面才会被看到。这个代码的光线跟踪器会把相机看不到的球面隐藏起来。</p><p>通过一个数据结构对球面建模，在数据结构中包含了球面的中心坐标(x, y, z)，半径radius，以及颜色值(r, g, b)。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">define</span> INF 2e10f</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">sphere</span>&#123;</span></span><br><span class="line">    <span class="type">float</span> r, g, b;</span><br><span class="line">    <span class="type">float</span> radius;</span><br><span class="line">    <span class="type">float</span> x, y, z;</span><br><span class="line">    __device__ <span class="type">float</span> <span class="title function_">hit</span><span class="params">(<span class="type">float</span> ox, <span class="type">float</span> oy, <span class="type">float</span> *n)</span>&#123;</span><br><span class="line">        <span class="type">float</span> dx = ox - x;</span><br><span class="line">        <span class="type">float</span> dy = oy - y;</span><br><span class="line">        <span class="keyword">if</span>(dx * dx + dy * dy &lt; radius * radius)&#123;</span><br><span class="line">            <span class="type">float</span> dz = sqrtf(radius * radius - dx * dx - dy * dy);</span><br><span class="line">            *n = dz / sqrtf(radius * radius);</span><br><span class="line">            <span class="keyword">return</span> dz + z;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> -INF;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这个结构中定义了一个方法hit(float ox, float oy, float *n)。对于来自(ox, oy)处像素的光线，这个方法将计算光线是否与这个球面相交。如果光线与球面相交，那么这个方法将计算从相机到光线命中球面处的距离。我们需要这个信息，因为当光线命中多个球面时，只有最接近相机的球面才会被看见。</p><p>main()函数遵循了与前面示例大致相同的代码结构。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;cuda.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;book.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;cpu_bitmap.h&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> rnd(x) (x * rand() / RAND_MAX)</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> SPHERES 20</span></span><br><span class="line"></span><br><span class="line">Sphere *s;</span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span>&#123;</span><br><span class="line">    <span class="comment">//记录起始时间</span></span><br><span class="line">    cudaEvent_ start, stop;</span><br><span class="line">    HANDLE_ERROR(cudaEventCreate(&amp;start));</span><br><span class="line">    HANDLE_ERROR(cudaEventCreate(&amp;stop));</span><br><span class="line">    HANDLE_ERROR(cudaEventRecord(start, <span class="number">0</span>));</span><br><span class="line">    </span><br><span class="line">    CPUBitmap <span class="title function_">bitmap</span><span class="params">(DIM, DIM)</span>;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">char</span> *dev_bitmap;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//在GPU上分配内存以计算输出位图</span></span><br><span class="line">    HANDLE_ERROR(cudaMalloc((<span class="type">void</span>**)&amp;dev_bitmap, bitmap.image_size()));</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//为Sphere数据集分配内存</span></span><br><span class="line">    HANDLE_ERROR(cudaMalloc((<span class="type">void</span>**)&amp;s, <span class="keyword">sizeof</span>(Sphere) * SPHERES));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在分配输入数据和输出数据的内存后，我们将随机地生成球面的中心坐标，颜色以及半径。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//分配临时内存，对其初始化，并复制到GPU上的内存，然后释放临时内存</span></span><br><span class="line">Sphere *temp_s = (Sphere*) <span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(Sphere) * SPHERES);</span><br><span class="line"><span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i &lt; SPHERES; i++)&#123;</span><br><span class="line">    temp_s[i].r = rnd(<span class="number">1.0f</span>);</span><br><span class="line">    temp_s[i].g = rnd(<span class="number">1.0f</span>);</span><br><span class="line">    temp_s[i].b = rnd(<span class="number">1.0f</span>);</span><br><span class="line">    temp_s[i].x = rnd(<span class="number">1000.0f</span>) - <span class="number">500</span>;</span><br><span class="line">    temp_s[i].y = rnd(<span class="number">1000.0f</span>) - <span class="number">500</span>;</span><br><span class="line">    temp_s[i].z = rnd(<span class="number">1000.0f</span>) - <span class="number">500</span>;</span><br><span class="line">    temp_s[i].radius = rnd(<span class="number">100.0f</span>) + <span class="number">20</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>当前，程序将生成一个包含20个球面的随机数组，但这个数量值是通过一个#define宏指定的，因此可以相应的做出调整。</p><p>通过cudaMemcpy()将这个球面数组复制到GPu，然后释放临时缓冲区。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">HANDLE_ERROR(cudaMemcpy(s, temps, <span class="keyword">sizeof</span>(Sphere) * SPHERES, cudaMemcpyHostToDevice));</span><br><span class="line"><span class="built_in">free</span>(temp_s);</span><br></pre></td></tr></table></figure><p>现在，输入数据位于GPU上，并且我们已经为输出数据分配好了空间，因此可以启动核函数。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//从球面数据汇总生成一张位图</span></span><br><span class="line">dim3 <span class="title function_">grids</span><span class="params">(DIM / <span class="number">16</span>, DIM / <span class="number">16</span>)</span>;</span><br><span class="line">dim3 <span class="title function_">threads</span><span class="params">(<span class="number">16</span>, <span class="number">16</span>)</span>;</span><br><span class="line">kernel&lt;&lt;&lt;grids, threads&gt;&gt;&gt;(dev_bitmap);</span><br></pre></td></tr></table></figure><p>这个核函数将执行光线跟踪计算并从输入的一组球面中为每个像素计算颜色数据。最后，我们把输出图像从GPU中复制回来，并显示它。我们还要释放所有已经分配但还未释放的内存。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//将位图从GPU复制回到CPU以显示</span></span><br><span class="line">HANDLE_ERROR(cudaMemcpy(bitmap.get_ptr(), dev_bitmap, bitmap.image_size(), cudaMemcpyDeviceToHost));</span><br><span class="line">bitmap.display_and_exit();</span><br><span class="line"></span><br><span class="line"><span class="comment">//释放内存</span></span><br><span class="line">cudaFree(dev_bitmap);</span><br><span class="line">cudaFree(s);</span><br></pre></td></tr></table></figure><p>下面的代码将介绍如何实现核函数的光线跟踪算法。每个线程都会为输出影像中的一个像素计算颜色值，因此我们遵循一种惯用的方式，计算每个线程对应的x坐标和y坐标，并且根据这两个坐标来计算输出缓冲区的偏移。此外，我们还将把图像坐标(x, y, z)偏移DIM&#x2F;2，这样z轴将穿过图像的中心。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">__global__ <span class="type">void</span> <span class="title function_">kernel</span><span class="params">(<span class="type">unsigned</span> <span class="type">char</span> *ptr)</span>&#123;</span><br><span class="line">    <span class="comment">//将threadIdx/BlockIdx映射到像素位置</span></span><br><span class="line">    <span class="type">int</span> x = threadIdx.x + blockIdx.x * blockDim.x;</span><br><span class="line">    <span class="type">int</span> y = threadIdx.y + blockIdx.y * blockDim.y;</span><br><span class="line">    </span><br><span class="line">    <span class="type">int</span> offset = x + y * blockDim.x * gridDim.x;</span><br><span class="line">    <span class="type">float</span> ox = x - DIM / <span class="number">2</span>;</span><br><span class="line">    <span class="type">float</span> oy = y - DIM / <span class="number">2</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//由于每条光线都需要判断与球面相交的情况，因此我们现在对球面数组进行迭代，并判断每个球面的命中情况。</span></span><br><span class="line">    <span class="type">float</span> r = <span class="number">0</span>, g = <span class="number">0</span>, b = <span class="number">0</span>;</span><br><span class="line">    <span class="type">float</span> maxz = -INF;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i &lt; SPHERES; i++)&#123;</span><br><span class="line">        <span class="type">float</span> n;</span><br><span class="line">        <span class="type">float</span> t = s[i].hit(ox, oy, &amp;n);</span><br><span class="line">        <span class="keyword">if</span>(t &gt; maxz)&#123;</span><br><span class="line">            <span class="type">float</span> fscale = n;</span><br><span class="line">            r = s[i].r * fscale;</span><br><span class="line">            g = s[i].g * fscale;</span><br><span class="line">            b = s[i].b * fscale;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//在判断了每个球面的相交情况后，可以将当前颜色值保存到输出图像中</span></span><br><span class="line">    ptr[offset * <span class="number">4</span> + <span class="number">0</span>] = (<span class="type">int</span>)(r * <span class="number">255</span>);</span><br><span class="line">    ptr[offset * <span class="number">4</span> + <span class="number">1</span>] = (<span class="type">int</span>)(g * <span class="number">255</span>);</span><br><span class="line">    ptr[offset * <span class="number">4</span> + <span class="number">1</span>] = (<span class="type">int</span>)(b * <span class="number">255</span>);</span><br><span class="line">    ptr[offset * <span class="number">4</span> + <span class="number">3</span>] = <span class="number">255</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="通过常量内存来实现光线跟踪"><a href="#通过常量内存来实现光线跟踪" class="headerlink" title="通过常量内存来实现光线跟踪"></a>通过常量内存来实现光线跟踪</h4><p>上述代码中并没有提到常量内存。下面的代码将使用常量内存来修改这个例子。由于常量内存无法修改，因此显然无法用常量内存来保存输出图像的数据。在这个示例中只有一个输入数据，即球面数组，因此应该将这个数据保存到常量内存中。</p><p>声明数组时，要在前面加上__constant__修饰符。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">__constant__ Sphere s[SPHERES];</span><br></pre></td></tr></table></figure><p>最初的示例中，我们声明了一个指针，然后通过cudaMalloc()来为指针分配GPU内存。当我们将其修改为常量内存时，同样要将这个声明修改为在常量内存中静态地分配空间。我们不再需要对球面数组调用cudaMalloc()或cudaFree()。而是在编译时为这个数组提交一个固定的大小。将main()函数修改为常量内存的代码如下:</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span>&#123;</span><br><span class="line">    CPUBitmap <span class="title function_">bitmap</span><span class="params">(DIM, DIM)</span>;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">char</span> *dev_bitmap;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//在GPU上分配内存以计算输出位图</span></span><br><span class="line">    HANDLE_ERROR(cudaMalloc((<span class="type">void</span>**)&amp;dev_bitmap, bitmap.image_size()));</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//分配临时内存，对其初始化，并复制到GPU上的内存，然后释放临时内存</span></span><br><span class="line">    Sphere *temp_s = (Sphere*) <span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(Sphere) * SPHERES);</span><br><span class="line"><span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i &lt; SPHERES; i++)&#123;</span><br><span class="line">        temp_s[i].r = rnd(<span class="number">1.0f</span>);</span><br><span class="line">        temp_s[i].g = rnd(<span class="number">1.0f</span>);</span><br><span class="line">        temp_s[i].b = rnd(<span class="number">1.0f</span>);</span><br><span class="line">        temp_s[i].x = rnd(<span class="number">1000.0f</span>) - <span class="number">500</span>;</span><br><span class="line">        temp_s[i].y = rnd(<span class="number">1000.0f</span>) - <span class="number">500</span>;</span><br><span class="line">        temp_s[i].z = rnd(<span class="number">1000.0f</span>) - <span class="number">500</span>;</span><br><span class="line">        temp_s[i].radius = rnd(<span class="number">100.0f</span>) + <span class="number">20</span>;</span><br><span class="line">&#125;</span><br><span class="line">    </span><br><span class="line">    HANDLE_ERROR(cudaMemcpyToSymbol(s, temp_s, <span class="keyword">sizeof</span>(Sphere) * SPHERES));</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">free</span>(temp_s);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//从球面数据中生成一张位图</span></span><br><span class="line">    dim3 <span class="title function_">grids</span><span class="params">(DIM / <span class="number">16</span>, DIM / <span class="number">16</span>)</span>;</span><br><span class="line">    dim3 <span class="title function_">threads</span><span class="params">(<span class="number">16</span>, <span class="number">16</span>)</span>;</span><br><span class="line">    kernel&lt;&lt;&lt;grids, threads&gt;&gt;&gt;(dev_bitmap);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//将位图从GPU复制回到CPU以显示</span></span><br><span class="line">    HANDLE_ERROR(cudaMemcpy(bitmap.get_ptr(), dev_bitmap, bitmap.image_size(), cudaMemcpyDeviceToHost));</span><br><span class="line">    bitmap.display_and_exit();</span><br><span class="line"></span><br><span class="line">    <span class="comment">//释放内存</span></span><br><span class="line">    cudaFree(dev_bitmap);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>cudaMemcpyToSymbol()与参数为cudaMemcpyHostToDevice()的cudaMemcpy()之间唯一差异时cudaMemcpyToSymbol()会复制到常量内存，而cudaMemcpy()会复制到全局内存。</p><h3 id="使用事件来测量性能"><a href="#使用事件来测量性能" class="headerlink" title="使用事件来测量性能"></a>使用事件来测量性能</h3><p>如何判断常量内存对程序性能有着多大影响？最简单的方式就是判断哪个版本的执行事件更短。使用CPU计时器或者操作系统中的某个计时器会带来各种延迟。为了测量GPU在某个任务上话费的时间，我们将使用CUDA的事件API。</p><p>CUDA中的事件本质上是一个GPU时间戳，这个时间戳是在用户指定的时间点上记录的。由于GPU本身支持记录时间戳，因此就避免了当使用CPU定时器来统计GPU执行的事件时可能遇到的诸多问题。比如，下面的代码开头告诉CUDA运行时记录当前的时间，首先创建一个事件，然后记录这个事件。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cudaEvent_t start;</span><br><span class="line">cudaEventCreate(&amp;start);</span><br><span class="line">cudaEventRecord(start, <span class="number">0</span>);</span><br></pre></td></tr></table></figure><p>要统计一段代码的执行时间，不仅要创建一个起始事件，还要创建一个结束事件。当在GPU上执行某个工作时，我们不仅要告诉CUDA运行时记录起始时间，还要记录结束时间:</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">cudaEvent_t start, stop;</span><br><span class="line">cudaEventCreate(&amp;start);</span><br><span class="line">cudaEventCreate(&amp;stop);</span><br><span class="line">cudaEventRecord(start, <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//在GPU执行一些工作</span></span><br><span class="line"></span><br><span class="line">cudaEventRecord(stop, <span class="number">0</span>);</span><br><span class="line">cudaEventSynchronize(stop); <span class="comment">//表示stop事件之前的所有GPU工作已经完成，可以安全读取在stop中保存的时间戳</span></span><br></pre></td></tr></table></figure><p>下面是对光线跟踪器进行性能测试的代码:</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span>&#123;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//记录起始时间</span></span><br><span class="line">    cudaEvent_t start, stop;</span><br><span class="line">cudaEventCreate(&amp;start);</span><br><span class="line">cudaEventCreate(&amp;stop);</span><br><span class="line">cudaEventRecord(start, <span class="number">0</span>);</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    CPUBitmap <span class="title function_">bitmap</span><span class="params">(DIM, DIM)</span>;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">char</span> *dev_bitmap;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//在GPU上分配内存以计算输出位图</span></span><br><span class="line">    HANDLE_ERROR(cudaMalloc((<span class="type">void</span>**)&amp;dev_bitmap, bitmap.image_size()));</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//分配临时内存，对其初始化，并复制到GPU上的内存，然后释放临时内存</span></span><br><span class="line">    Sphere *temp_s = (Sphere*) <span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(Sphere) * SPHERES);</span><br><span class="line"><span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i &lt; SPHERES; i++)&#123;</span><br><span class="line">        temp_s[i].r = rnd(<span class="number">1.0f</span>);</span><br><span class="line">        temp_s[i].g = rnd(<span class="number">1.0f</span>);</span><br><span class="line">        temp_s[i].b = rnd(<span class="number">1.0f</span>);</span><br><span class="line">        temp_s[i].x = rnd(<span class="number">1000.0f</span>) - <span class="number">500</span>;</span><br><span class="line">        temp_s[i].y = rnd(<span class="number">1000.0f</span>) - <span class="number">500</span>;</span><br><span class="line">        temp_s[i].z = rnd(<span class="number">1000.0f</span>) - <span class="number">500</span>;</span><br><span class="line">        temp_s[i].radius = rnd(<span class="number">100.0f</span>) + <span class="number">20</span>;</span><br><span class="line">&#125;</span><br><span class="line">    </span><br><span class="line">    HANDLE_ERROR(cudaMemcpyToSymbol(s, temp_s, <span class="keyword">sizeof</span>(Sphere) * SPHERES));</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">free</span>(temp_s);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//从球面数据中生成一张位图</span></span><br><span class="line">    dim3 <span class="title function_">grids</span><span class="params">(DIM / <span class="number">16</span>, DIM / <span class="number">16</span>)</span>;</span><br><span class="line">    dim3 <span class="title function_">threads</span><span class="params">(<span class="number">16</span>, <span class="number">16</span>)</span>;</span><br><span class="line">    kernel&lt;&lt;&lt;grids, threads&gt;&gt;&gt;(dev_bitmap);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//将位图从GPU复制回到CPU以显示</span></span><br><span class="line">    HANDLE_ERROR(cudaMemcpy(bitmap.get_ptr(), dev_bitmap, bitmap.image_size(), cudaMemcpyDeviceToHost));</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="comment">//获得结束时间，并显示计时结果</span></span><br><span class="line">    HANDLE_ERROR(cudaEventRecord(stop, <span class="number">0</span>));</span><br><span class="line">    HANDLE_ERROR(cudaEventSynchronize(stop));</span><br><span class="line">    </span><br><span class="line">    <span class="type">float</span> elapsedTime;</span><br><span class="line">    HANDLE_ERROR(cudaEventElapsedTime(&amp;elapsedTime, start, stop));</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Time to generate: %3.1f ms\n&quot;</span>, elapsedTime);</span><br><span class="line">    </span><br><span class="line">    HANDLE_ERROR(cudaEventDestroy(start));</span><br><span class="line">    HANDLE_ERROR(cudaEventDestroy(stop));    </span><br><span class="line">    </span><br><span class="line">    <span class="comment">//显示位图</span></span><br><span class="line">    bitmap.display_and_exit();</span><br><span class="line"></span><br><span class="line">    <span class="comment">//释放内存</span></span><br><span class="line">    cudaFree(dev_bitmap);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> CUDA </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>CUDA学习(三)</title>
      <link href="/2024/02/21/CUDA%E5%AD%A6%E4%B9%A0-%E4%B8%89/"/>
      <url>/2024/02/21/CUDA%E5%AD%A6%E4%B9%A0-%E4%B8%89/</url>
      
        <content type="html"><![CDATA[<p>这一章将介绍线程块以及线程之间的通信机制和同步机制。</p><p>在GPU启动并行代码的实现方法是告诉CUDA运行时启动核函数的多个并行副本。我们将这些并行副本称为线程块(Block)。</p><p>CUDA运行时将把这些线程块分解为多个线程。当需要启动多个并行线程块时，只需将尖括号中的第一个参数由1改为想要启动的线程块数量。</p><p>在尖括号中，第二个参数表示CUDA运行时在每个线程块中创建的线程数量。假设尖括号中的变量为&lt;&lt;&lt;N, M&gt;&gt;&gt;总共启动的线程数量可以按照以下公式计算:<br>$$<br>N个线程块 * M个线程&#x2F;线程块 &#x3D; N*M个并行线程<br>$$</p><span id="more"></span><h3 id="使用线程实现GPU上的矢量求和"><a href="#使用线程实现GPU上的矢量求和" class="headerlink" title="使用线程实现GPU上的矢量求和"></a>使用线程实现GPU上的矢量求和</h3><p>在之前的代码中，我们才去的时调用N个线程块，每个线程块对应一个线程<code>add&lt;&lt;&lt;N, 1&gt;&gt;&gt;(dev_a, dev_b, dev_c);</code>。</p><p>如果我们启动N个线程，并且所有线程都在一个线程块中，则可以表示为<code>add&lt;&lt;&lt;1, N&gt;&gt;&gt;(dev_a, dev_b, dev_c);</code>。此外，因为只有一个线程块，我们需要通过线程索引来对数据进行索引(而不是线程块索引)，需要将<code>int tid = blockIdx.x;</code>修改为<code>int tid = threadIdx.x;</code></p><h3 id="在GPU上对更长的矢量求和"><a href="#在GPU上对更长的矢量求和" class="headerlink" title="在GPU上对更长的矢量求和"></a>在GPU上对更长的矢量求和</h3><p>对于启动核函数时每个线程块中的线程数量，硬件也进行了限制。具体来说，最大的线程数量不能超过设备树形结构中maxThreadsPerBlock域的值。对目前的GPU来说一个线程块最多有1024个线程。如果要通过并行线程对长度大于1024的矢量进行相加的话，就需要将线程与线程块结合起来才能实现。</p><p>此时，计算索引可以表示为:</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> tid = threadIdx.x + blockIdx.x * blockDim.x;</span><br></pre></td></tr></table></figure><p>blockDim保存的事线程块中每一维的线程数量，由于使用的事一维线程块，因此只用到blockDim.x。</p><p>此外，gridDim是二维的，而blockDim是三维的。</p><p>假如我们使用多个线程块处理N个并行线程，每个线程块处理的线程数量为128，那样可以启动N&#x2F;128个线程块。然而问题在于，当N小于128时，比如127，那么N&#x2F;128等于0，此时将会启动0个线程块。所以我们希望这个除法能够向上取整。我们可以不用调用 ceil()函数，而是将计算改为(N+127)&#x2F;N。因此，这个例子调用核函数可以写为:</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">add&lt;&lt;&lt;(N + <span class="number">127</span>) / <span class="number">128</span>, <span class="number">128</span>&gt;&gt;&gt;(dev_a, dev_b, dev_c);</span><br></pre></td></tr></table></figure><p>当N不是128的整数倍时，将启动过多的线程。然而，在核函数中已经解决了这个问题。在访问输入数组和输出数组之前，必须检查线程的便宜是否位于0到N之间。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span>(tid &lt; N)&#123;</span><br><span class="line">c[tid] = a[tid] + b[tid];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>因此，当索引越过数组的边界时，核函数将自动停止执行计算。核函数不会对越过数组边界的内存进行读取或者写入。</p><h3 id="在GPU上对任意长度的矢量求和"><a href="#在GPU上对任意长度的矢量求和" class="headerlink" title="在GPU上对任意长度的矢量求和"></a>在GPU上对任意长度的矢量求和</h3><p>当矢量的长度很长时，我们可以让每一个线程执行多个矢量相加。例如</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">__global__ <span class="type">void</span> <span class="title function_">add</span><span class="params">(<span class="type">int</span> *a, <span class="type">int</span> *b, <span class="type">int</span> *c)</span>&#123;</span><br><span class="line">    <span class="type">int</span> tid = threadIdx.x + blockIdx.x * blockDim.x;</span><br><span class="line">    <span class="keyword">while</span>(tid &lt; N)&#123;</span><br><span class="line">        c[tid] = a[tid] + b[tid];</span><br><span class="line">        tid += blockDim.x * gridDim.x;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>当每个线程计算完当前索引上的任务后，接着就需要对索引进行递增，其中递增的步长为线程格中正在运行的线程数量。这个数值等于每个线程块中的线程数量乘上线程格中线程块的数量，即blockDim.x * gridDim.x。</p><h3 id="共享内存和同步"><a href="#共享内存和同步" class="headerlink" title="共享内存和同步"></a>共享内存和同步</h3><p>CUDA C编译器对共享内存中的变量与普通变量分别采取不同的处理方式。对于在GPU上启动的每个线程块，CUDA C编译器都将创建该变量的一个副本，线程块中的每个线程都共享这块内存，但线程却无法看到也不能修改其他线程块的变量副本。这就实现了一个非常好的方式，<strong>使得一个线程块中的多个线程能够在计算上进行通信和协作</strong>。</p><p>而且，共享内存缓冲区驻留在物理GPU上，而不是驻留在GPU之外的系统内存中。因此，<strong>在访问共享内存时的延迟要远远低于访问普通缓冲区的延迟</strong>，使得共享内存像每个线程块的高速缓存或者中间结果暂存器那样高效。</p><p>如果想要实现线程之间通信，那么还需要一种机制来实现线程之间的同步。例如，如果线程A将一个值写入到共享内存，并且我们希望线程B对这个值进行一些操作，那么只有当线程A的写入操作完成之后，线程B才能开始执行它的操作。如果没有同步，那么将发生竞态条件(race condition)。</p><p>下面将通过一个矢量的点积运算来详细介绍共享内存和同步。矢量点积运算为矢量相乘结束后将值相加起来以得到一个标量输出值。例如对两个包含4个元素的矢量进行点积运算:<br>$$<br>(x_1, x_2, x_3, x_4) * (y_1, y_2, y_3, y_4) &#x3D; x_1y_1 + x_2y_2 + x_3y_3 + x_4y_4<br>$$<br>由于最终结果是所有乘积的总和，因此每个线程要保存它所计算的乘积的加和。下面代码实现了点积函数的第一个步骤:</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;book.h&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> imin(a, b) (a &lt; b ? a : b)</span></span><br><span class="line"></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="number">33</span> * <span class="number">1024</span>;</span><br><span class="line"><span class="type">const</span> <span class="type">int</span> threadsPerBlock = <span class="number">256</span>;</span><br><span class="line"></span><br><span class="line">__global__ <span class="type">void</span> <span class="title function_">dot</span><span class="params">(<span class="type">float</span> *a, <span class="type">float</span> *b, <span class="type">float</span> *c)</span>&#123;</span><br><span class="line">    __shared__ <span class="type">float</span> cache[threadsPerBlock];</span><br><span class="line">    <span class="type">int</span> tid = threadIdx.x + blockIdx.x * blockDim.x;</span><br><span class="line">    <span class="type">int</span> cacheIndex = threadIdx.x;</span><br><span class="line">    <span class="type">float</span> temp = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span>(tid &lt; N)&#123;</span><br><span class="line">        temp += a[tid] * b[tid];</span><br><span class="line">        tid += blockDim.x * gridDim.x;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//设置cache中相应位置上的值</span></span><br><span class="line">    cache[cacheIndex] = temp;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>代码中声明了一个共享内存缓冲区，名字为cache。这个缓冲区将保存每个线程计算的加和值。我们将cache数组的大小声明为threadsPerBlock，这样线程块中每个线程都能将它计算的临时结果保存到某个位置上。之前在分配全局内存时，我们为每个执行核函数的线程都分配了足够的内存，即线程块的数量乘以threadsPerBlock。但对于共享变量来说，由于编译器将为每个线程块生成共享变量的一个副本，因此只需根据线程块中线程的数量来分配内存。</p><p>我们需要将cache中所有的值相加起来。在执行这个运算时，需要通过一个线程来读取保存在cache中的值。由于race condition，我们需要使用下面的代码来确保对所有共享数组cache[]的写入操作在读组cache之前完成:</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//对线程块中的线程进行同步</span></span><br><span class="line">__syncthreads();</span><br></pre></td></tr></table></figure><p>这个函数调用将确保线程块中的每个线程都执行完__syncthreads()前面的语句后，才会执行下一条语句。</p><p>这时，我们可以将其中的值相加起来(称为归约Reduction)。代码的基本思想是每个线程将cache[]中的两个值相加起来，然后将结果保存回cache[]。由于每个线程都将两个值合并为一个值，那么在完成这个步骤后，得到的结果数量就是计算开始时数值数量的一半。下一个步骤中我们对这一半数值执行相同的操作，在将这种操作执行log2(threadsPerBlock)步骤后，就能得到cache[]中所有值的总和。对于这个示例来说，我们在每个线程块中使用了256个线程，因此需要8次迭代将cache[]中的256个值归约为1个值。这个归约过程的实现可以表示为以下代码:</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//对于归约运算来说，以下代码要求threadsPerBlock必须时2的指数</span></span><br><span class="line"><span class="type">int</span> i = blockDim.x / <span class="number">2</span>;</span><br><span class="line"><span class="keyword">while</span>(i != <span class="number">0</span>)&#123;</span><br><span class="line">    <span class="keyword">if</span>(cacheIndex &lt; i)&#123;</span><br><span class="line">        cache[cacheIndex] += cache[cacheIndex + i];</span><br><span class="line">    &#125;</span><br><span class="line">    __syncthreads();</span><br><span class="line">    i /= <span class="number">2</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>再结束了while()循环后，每个线程块都得到了一个值，这个值位于cache[]的第一个元素中，并且就等于该线程块中两两元素乘积的加和。然后，我们将这个值保存到全局内存并结束核函数:</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span>(cacheIndex == <span class="number">0</span>)&#123;</span><br><span class="line">    c[blockIdx.x] = cache[<span class="number">0</span>];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>只让cacheIndex为0的线程执行保存操作时因为每个线程块只有一个值写入到全局内存，因此每个线程块只需要一个线程来执行这个操作。最后，由于每个线程块都只写入一个值到全局数据c[]中，因此可以通过blockIdx来索引这个值。</p><p>点积运算的最后一个步骤就是计算c[]中所有元素的总和。像GPU这种大规模的并行机器在执行最后的归约步骤时，通常会浪费计算资源，因为此时的数据集往往会非常小。因此，我们可以将执行控制返回给主机，并且由CPU来完成最后一个加法步骤。</p><p>下面给出了完整的代码实现:</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;book.h&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> imin(a, b) (a &lt; b? a : b)</span></span><br><span class="line"></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="number">33</span> * <span class="number">1024</span>;</span><br><span class="line"><span class="type">const</span> <span class="type">int</span> threadsPerBlock = <span class="number">256</span>;</span><br><span class="line"><span class="type">const</span> <span class="type">int</span> blocksPerGrid = imin(<span class="number">32</span>, (N + threadsPerBlock - <span class="number">1</span>) / threadsPerBlock);</span><br><span class="line"></span><br><span class="line">__global__ <span class="type">void</span> <span class="title function_">dot</span><span class="params">(<span class="type">float</span> *a, <span class="type">float</span> *b, <span class="type">float</span> *c)</span>&#123;</span><br><span class="line">    __shared__ <span class="type">float</span> cache[threadsPerBlock];</span><br><span class="line">    <span class="type">int</span> tid = threadIdx.x + blockIdx.x * blockDim.x;</span><br><span class="line">    <span class="type">int</span> cacheIndex = threadIdx.x;</span><br><span class="line">    </span><br><span class="line">    <span class="type">float</span> temp = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span>(tid &lt; N)&#123;</span><br><span class="line">        temp += a[tid] * b[tid];</span><br><span class="line">        tid += blockDim.x * gridDim.x;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//设置cache中相应位置上的值</span></span><br><span class="line">    cache[cacheIndex] = temp;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//对线程块中的线程进行同步</span></span><br><span class="line">    __syncthreads();</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//对于归约来说，以下代码要求threadsPerBlock必须是2的指数</span></span><br><span class="line">    <span class="type">int</span> i = blockDim.x / <span class="number">2</span>;</span><br><span class="line">    <span class="keyword">while</span>(i != <span class="number">0</span>)&#123;</span><br><span class="line">        <span class="keyword">if</span>(cacheIndex &lt; i)&#123;</span><br><span class="line">            cache[cacheIndex] += cache[cacheIndex + i];</span><br><span class="line">        &#125;</span><br><span class="line">        __syncthreads(); <span class="comment">//循环中更新了变量cache，所以需要在下一次循环前进行同步。该同步语句需要所有的线程都必须运行才行。如果有线程不能运行这一处代码，会导致其他线程永远等待。</span></span><br><span class="line">        i /= <span class="number">2</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(cacheIndex == <span class="number">0</span>)&#123;</span><br><span class="line">        c[blockIndex.x] = cache[<span class="number">0</span>];</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span>&#123;</span><br><span class="line">    <span class="type">float</span> *a, *b, c, *partial_c;</span><br><span class="line">    <span class="type">float</span> *dev_c, *dev_b, *dev_partial_c;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//在CPU上分配内存</span></span><br><span class="line">    a = (<span class="type">float</span>*) <span class="built_in">malloc</span>(N*<span class="keyword">sizeof</span>(<span class="type">float</span>));</span><br><span class="line">    b = (<span class="type">float</span>*) <span class="built_in">malloc</span>(N*<span class="keyword">sizeof</span>(<span class="type">float</span>));</span><br><span class="line">    partial_c = (<span class="type">float</span>*) <span class="built_in">malloc</span>(blocksPerGrid * <span class="keyword">sizeof</span>(<span class="type">float</span>));</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//在GPU上分配内存</span></span><br><span class="line">    HANDLE_ERROR(cudaMalloc((<span class="type">void</span>**)&amp;dev_a, N * <span class="keyword">sizeof</span>(<span class="type">float</span>)));</span><br><span class="line">    HANDLE_ERROR(cudaMalloc((<span class="type">void</span>**)&amp;dev_b, N * <span class="keyword">sizeof</span>(<span class="type">float</span>)));</span><br><span class="line">    HANDLE_ERROR(cudaMalloc((<span class="type">void</span>**)&amp;dev_partial_c, blocksPerGrid * <span class="keyword">sizeof</span>(<span class="type">float</span>)));</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//填充主机内存</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i &lt; N; i++)&#123;</span><br><span class="line">        a[i] = i;</span><br><span class="line">        b[i] = i * <span class="number">2</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//将数组&quot;a&quot;和&quot;b&quot;复制到GPU</span></span><br><span class="line">    HANDLE_ERROR(cudaMemcpy(dev_a, a, N * <span class="keyword">sizeof</span>(<span class="type">float</span>), cudaMemcpyHostToDevice));</span><br><span class="line">    HANDLE_ERROR(cudaMemcpy(dev_b, b, N * <span class="keyword">sizeof</span>(<span class="type">float</span>), cudaMemcpyHostToDevice));</span><br><span class="line">    dot&lt;&lt;&lt;blocksPerGrid, threadsPerBlock&gt;&gt;&gt;(dev_a, dev_b, dev_partial_c);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//将数组&quot;c&quot;从GPU复制到CPU</span></span><br><span class="line">    HANDLE_ERROR(cudaMemcpy(partial_c, dev_partial_c, blocksPerGrid * <span class="keyword">sizeof</span>(<span class="type">float</span>), cudaMemcpyDeviceToHost));</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//在CPU上完成最终的求和运算</span></span><br><span class="line">    c = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i &lt; blocksPerGrid; i++)&#123;</span><br><span class="line">        c += partial_c[i];</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="meta">#<span class="keyword">define</span> sum_squares(x) (x * (x + 1) * (2 * x + 1) / 6)</span></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Does GPU value %.6g = %.6g? \n&quot;</span>, c, <span class="number">2</span> * sum_square((<span class="type">float</span>) (N - <span class="number">1</span>)));</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//释放GPU上的内存</span></span><br><span class="line">    cudaFree(dev_a);</span><br><span class="line">    cudaFree(dev_b);</span><br><span class="line">    cudaFree(dev_partial_c);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//释放CPU上的内存</span></span><br><span class="line">    <span class="built_in">free</span>(a);</span><br><span class="line">    <span class="built_in">free</span>(b);</span><br><span class="line">    <span class="built_in">free</span>(partial_c);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> CUDA </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>CUDA学习(二)</title>
      <link href="/2024/02/21/CUDA%E5%AD%A6%E4%B9%A0-%E4%BA%8C/"/>
      <url>/2024/02/21/CUDA%E5%AD%A6%E4%B9%A0-%E4%BA%8C/</url>
      
        <content type="html"><![CDATA[<p>这一部分将介绍CUDA的并行编程方式</p><span id="more"></span><h2 id="矢量求和运算"><a href="#矢量求和运算" class="headerlink" title="矢量求和运算"></a>矢量求和运算</h2><p>假设有两组数据，我们需要将这两组数据中对应的元素两两相加，并将结果保存在第三个数组中。</p><h3 id="基于CPU的矢量求和"><a href="#基于CPU的矢量求和" class="headerlink" title="基于CPU的矢量求和"></a>基于CPU的矢量求和</h3><p>首先，下面的代码是通过传统的C代码来实现这个求和运算</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;book.h&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> N 10</span></span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">add</span><span class="params">(<span class="type">int</span> *a, <span class="type">int</span> *b, <span class="type">int</span> *c)</span>&#123;</span><br><span class="line"><span class="type">int</span> tid = <span class="number">0</span>;<span class="comment">//这是第0个CPU，因此索引从0开始</span></span><br><span class="line">    <span class="keyword">while</span>(tid &lt; N)&#123;</span><br><span class="line">        c[tid] = a[tid] + b[tid];</span><br><span class="line">        tid += <span class="number">1</span>;<span class="comment">//由于只有一个CPU，因此每次递增1</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span>&#123;</span><br><span class="line">    <span class="type">int</span> a[N], b[N], c[N];</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//在CPU上为数组&quot;a&quot;和&quot;b&quot;赋值</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i &lt; N; i++)&#123;</span><br><span class="line">        a[i] = -i;</span><br><span class="line">        b[i] = i * i;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    add(a, b, c);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//显示结果</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i &lt; N; i++)&#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;%d + %d = %d\n&quot;</span>, a[i], b[i], c[i]);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>add()中使用while循环而不是for循环是为了代码能够在拥有多个CPU或者多个CPU核的系统上并行运行，比如双核处理器上可以将每次递增的大小改为2。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//第一个CPU核</span></span><br><span class="line"><span class="type">void</span> <span class="title function_">add</span><span class="params">(<span class="type">int</span> *a, <span class="type">int</span> *b, <span class="type">int</span> *c)</span>&#123;</span><br><span class="line">    <span class="type">int</span> tid = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span>(tid &lt; N)&#123;</span><br><span class="line">        c[tid] = a[tid] + b[tid];</span><br><span class="line">        tid += <span class="number">2</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//第二个CPU核</span></span><br><span class="line"><span class="type">void</span> <span class="title function_">add</span><span class="params">(<span class="type">int</span> *a, <span class="type">int</span> *b, <span class="type">int</span> *c)</span>&#123;</span><br><span class="line">    <span class="type">int</span> tid = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">while</span>(tid &lt; N)&#123;</span><br><span class="line">        c[tid] = a[tid] + b[tid];</span><br><span class="line">        tid += <span class="number">2</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="基于GPU的矢量求和"><a href="#基于GPU的矢量求和" class="headerlink" title="基于GPU的矢量求和"></a>基于GPU的矢量求和</h3><p>下面是基于GPU的矢量求和代码</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;book.h&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> N 10</span></span><br><span class="line"></span><br><span class="line">__global__ <span class="title function_">add</span><span class="params">(<span class="type">int</span> *dev_a, <span class="type">int</span> *dev_c, <span class="type">int</span> *dev_c)</span>&#123;</span><br><span class="line">    <span class="type">int</span> tid = blockIdx.x; <span class="comment">//计算该索引处的数据</span></span><br><span class="line">    <span class="keyword">if</span>(tid &lt; N)&#123;</span><br><span class="line">        c[tid] = a[tid] + b[tid];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span>&#123;</span><br><span class="line">    <span class="type">int</span> a[N], b[N], c[N];</span><br><span class="line">    <span class="type">int</span> *dev_a, *dev_b, *dev_c;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//在GPU上分配内存</span></span><br><span class="line">    HANDLE_ERROR(cudaMalloc((<span class="type">void</span>**)&amp;dev_a, N * <span class="keyword">sizeof</span>(<span class="type">int</span>)));</span><br><span class="line">    HANDLE_ERROR(cudaMalloc((<span class="type">void</span>**)&amp;dev_b, N * <span class="keyword">sizeof</span>(<span class="type">int</span>)));</span><br><span class="line">    HANDLE_ERROR(cudaMalloc((<span class="type">void</span>**)&amp;dev_c, N * <span class="keyword">sizeof</span>(<span class="type">int</span>)));</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//在CPU上为数组&quot;a&quot;和&quot;b&quot;赋值</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i &lt; N; i++)&#123;</span><br><span class="line">        a[i] = -i;</span><br><span class="line">        b[i] = i * i;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//将数组&quot;a&quot;和&quot;b&quot;复制到GPU</span></span><br><span class="line">    HANDLE_ERROR(cudaMemcpy(dev_a, a, N * <span class="keyword">sizeof</span>(<span class="type">int</span>), cudaMemcpyHostToDevice));</span><br><span class="line">    HANDLE_ERROR(cudaMemcpy(dev_b, b, N * <span class="keyword">sizeof</span>(<span class="type">int</span>), cudaMemcpyHostToDevice));</span><br><span class="line">    </span><br><span class="line">    add&lt;&lt;&lt;N, <span class="number">1</span>&gt;&gt;&gt;(dev_a, dev_b, dev_c);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//将数组&quot;c&quot;从GPU复制到CPU</span></span><br><span class="line">    HANDLE_ERROR(cudaMemcpy(c, dev_c, N * <span class="keyword">sizeof</span>(<span class="type">int</span>), cudaMemcpyDeviceToHost));</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//显示结果</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i &lt; N; i++)&#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;%d + %d = %d\n&quot;</span>, a[i], b[i], c[i]);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//释放在GPU上分配的内存</span></span><br><span class="line">    cudaFree(dev_a);</span><br><span class="line">    cudaFree(dev_b);</span><br><span class="line">    cudaFree(dev_c);</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在示例代码中，调用add函数的尖括号内的数值是&lt;&lt;&lt;N, 1&gt;&gt;&gt;，其中第一个参数表示设备在执行核函数时使用的并行线程块的数量。比如如果制定的事kernel&lt;&lt;&lt;256, 1&gt;&gt;&gt;()，那么将有256个线程块在GPU上运行。</p><p>在add函数里面，我们可以使用blockIdx.x获取具体的线程块(blockIdx是一个内置变量，不需要定义它)，通过这种方式可以让不同的线程块并行执行数组的矢量相加。</p><p>下一章将会详细解释线程块以及线程之间的通信机制和同步机制。</p>]]></content>
      
      
      <categories>
          
          <category> CUDA </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>CUDA学习(一)</title>
      <link href="/2024/02/20/CUDA%E5%AD%A6%E4%B9%A0-%E4%B8%80/"/>
      <url>/2024/02/20/CUDA%E5%AD%A6%E4%B9%A0-%E4%B8%80/</url>
      
        <content type="html"><![CDATA[<p><strong>参考书目:</strong> GPU高性能编程CUDA实战</p><p><strong>书目网页链接:</strong> <a href="https://hpc.pku.edu.cn/docs/20170829223652566150.pdf">https://hpc.pku.edu.cn/docs/20170829223652566150.pdf</a></p><p>该博客参考于上述书籍，虽然书有一点老，但是作为初学者而言仍然能学到很多东西。</p><span id="more"></span><h2 id="CUDA-C简介"><a href="#CUDA-C简介" class="headerlink" title="CUDA C简介"></a>CUDA C简介</h2><p>首先来看一个CUDA C的示例:</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;../common/book.h&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span>&#123;</span><br><span class="line">  prinf(<span class="string">&quot;Hello World!\n&quot;</span>);</span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这个示例只是为了说明，CUDA C与熟悉的标准C在很大程度上是没有区别的。</p><h3 id="核函数调用"><a href="#核函数调用" class="headerlink" title="核函数调用"></a>核函数调用</h3><p>在GPU设备上执行的函数通常称为核函数(Kernel)</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"></span><br><span class="line">__global__ <span class="type">void</span> <span class="title function_">kernel</span><span class="params">()</span>&#123;</span><br><span class="line">  </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span>&#123;</span><br><span class="line">  kernel&lt;&lt;&lt;<span class="number">1</span>, <span class="number">1</span>&gt;&gt;&gt;();</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;Hello World!\n&quot;</span>);</span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>跟之前的代码相比多了两处</p><ul><li>一个空的函数kernel()，并且带有修饰符__global__。</li><li>对这个空函数的调用，并且带有修饰字符&lt;&lt;&lt;1, 1&gt;&gt;&gt;。</li></ul><p>这个__global__可以认为是告诉编译器，函数应该编译为在设备而不是在主机上运行。函数kernel()将被交给编译器设备代码的编译器，而main()函数将被交给主机编译器。</p><h3 id="传递参数"><a href="#传递参数" class="headerlink" title="传递参数"></a>传递参数</h3><p>以下是对上述代码的进一步修改，可以实现将参数传递给核函数</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;book.h&quot;</span></span></span><br><span class="line"></span><br><span class="line">__global__ <span class="type">void</span> <span class="title function_">add</span><span class="params">(<span class="type">int</span> a, <span class="type">int</span> b, <span class="type">int</span>* c)</span>&#123;</span><br><span class="line">    *c = a + b;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;c is %d\n&quot;</span>, *c);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">(<span class="type">void</span>)</span>&#123;</span><br><span class="line">    <span class="type">int</span> c = <span class="number">0</span>;</span><br><span class="line">    <span class="type">int</span>* dev_c;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;original c is %d\n&quot;</span>, c);</span><br><span class="line"></span><br><span class="line">    HANDLE_ERROR(cudaMalloc((<span class="type">void</span>**)&amp;dev_c, <span class="keyword">sizeof</span>(<span class="type">int</span>)));</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    add&lt;&lt;&lt;<span class="number">1</span>, <span class="number">1</span>&gt;&gt;&gt;(<span class="number">2</span>, <span class="number">7</span>, dev_c);</span><br><span class="line">    HANDLE_ERROR(cudaMemcpy(&amp;c, dev_c, <span class="keyword">sizeof</span>(<span class="type">int</span>), cudaMemcpyDeviceToHost));</span><br><span class="line"></span><br><span class="line">    cudaFree(dev_c);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;2 + 7 = %d\n&quot;</span>, c);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其中”book.h”包含了HANDLE_ERROR，也可以不使用”book.h”而是在代码中添加HANDLE_ERROR函数。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="type">static</span> <span class="type">void</span> <span class="title function_">HandleError</span><span class="params">( cudaError_t err,</span></span><br><span class="line"><span class="params">                         <span class="type">const</span> <span class="type">char</span> *file,</span></span><br><span class="line"><span class="params">                         <span class="type">int</span> line )</span> &#123;</span><br><span class="line">    <span class="keyword">if</span> (err != cudaSuccess) &#123;</span><br><span class="line">        <span class="built_in">printf</span>( <span class="string">&quot;%s in %s at line %d\n&quot;</span>, cudaGetErrorString( err ),</span><br><span class="line">                file, line );</span><br><span class="line">        <span class="built_in">exit</span>( EXIT_FAILURE );</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">#<span class="keyword">define</span> HANDLE_ERROR( err ) (HandleError( err, __FILE__, __LINE__ ))</span></span><br><span class="line"></span><br><span class="line">__global__ <span class="type">void</span> <span class="title function_">add</span><span class="params">(<span class="type">int</span> a, <span class="type">int</span> b, <span class="type">int</span>* c)</span>&#123;</span><br><span class="line">    *c = a + b;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;c is %d\n&quot;</span>, *c);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">(<span class="type">void</span>)</span>&#123;</span><br><span class="line">    <span class="type">int</span> c = <span class="number">0</span>;</span><br><span class="line">    <span class="type">int</span>* dev_c;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;original c is %d\n&quot;</span>, c);</span><br><span class="line"></span><br><span class="line">    HANDLE_ERROR(cudaMalloc((<span class="type">void</span>**)&amp;dev_c, <span class="keyword">sizeof</span>(<span class="type">int</span>)));</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    add&lt;&lt;&lt;<span class="number">1</span>, <span class="number">1</span>&gt;&gt;&gt;(<span class="number">2</span>, <span class="number">7</span>, dev_c);</span><br><span class="line">    HANDLE_ERROR(cudaMemcpy(&amp;c, dev_c, <span class="keyword">sizeof</span>(<span class="type">int</span>), cudaMemcpyDeviceToHost));</span><br><span class="line"></span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;2 + 7 = %d\n&quot;</span>, c);</span><br><span class="line">    cudaFree(dev_c);</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>cudaMalloc()用来分配内存，这个函数的调用行为非常类似于标准C函数的malloc()，但该函数作用是告诉CUDA运行时在设备上分配内存。<ul><li>第一个参数是一个指针，指向用于保存新分配内存地址的变量。第二个参数是分配内存的大小。</li><li>该函数返回的类型是void*。</li><li>不能使用标准C的free()函数来释放cudaMallocc()分配的内存。要释放cudaMalloc()分配的内存，需要调用cudaFree()。</li></ul></li><li>HANDLE_ERROR()是定义的一个宏，作为辅助代码的一部分，用来判断函数调用是否返回了一个错误值，如果是的话，将输出相应的错误消息。</li><li>在主机代码中可以通过调用cudaMemcpy()来访问设备上的内存。<ul><li>第一个参数是目标(target)指针，第二个参数是源(source)指针，第三个参数分配内存大小。第四个参数则是指定设备内存指针。</li><li>第四个参数一般有cudaMemcpyDeviceToHost，cudaMemcpyHostToDevice, cudaMemcpyDeviceToDevice三种。cudaMemcpyDeviceToHost说明我们将设备内存指针的数据传递给主机内存指针，此时第一个参数指针是在主机上，第二个参数指针是在设备上。cudaMemcpyHostToDevice说明我们将主机内存指针的数据传递给设备内存指针，此时第一个参数指针是在设备上，第二个参数指针是在主机上。此外还可以通过传递参数cudaMemcpyDeviceToDevice莱高速运行时这两个指针都在设备上。如果源指针和目标指针都在主机上，则可以直接调用memcpy()函数。</li></ul></li></ul><h3 id="查询设备"><a href="#查询设备" class="headerlink" title="查询设备"></a>查询设备</h3><p>我们可以使用cudaGetDeviceCount()来查询设备数量(比如GPU数量)。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> count;</span><br><span class="line">HANDLE_ERROR(cudaGetDeviceCount(&amp;count));</span><br></pre></td></tr></table></figure><p>CUDA设备属性包含很多信息，可以在书上或者NVIDIA官方网站上查到。</p>]]></content>
      
      
      <categories>
          
          <category> CUDA </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2024/02/16/hello-world/"/>
      <url>/2024/02/16/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><span id="more"></span><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
